

## ğŸ†• TÃ³m táº¯t cÃ¡c thay Ä‘á»•i chÃ­nh (Proposed vs Original)

### 1. **Instruction Encoder** (HoÃ n toÃ n má»›i)
| Component | Chi tiáº¿t |
|-----------|----------|
| **T5 Tokenizer** | `google/flan-t5-large` tokenizer |
| **Flan-T5-Large Encoder** | FROZEN, khÃ´ng train |
| **Attention Pooling** | Learnable query `[1, 1, 1024]` + MultiheadAttention (8 heads) |
| **Linear Projection** | `1024 â†’ 1024` vá»›i Xavier init |
| **Output** | `instruction_emb [Batch, 1024]` |

### 2. **CustomLlamaModel** (Thay tháº¿ LlamaModel gá»‘c)
| Component | Chi tiáº¿t |
|-----------|----------|
| **CustomLlamaDecoderLayer** | 30 layers, má»—i layer cÃ³ 2 adapters |
| **AdaRMSNormAdapter** | Adaptive Layer Normalization |
| **Adapter Formula** | `output = RMSNorm(x) Ã— (1 + Î³_ada) + Î²_ada` |
| **Zero-Init** | Last layer cá»§a adapter Ä‘Æ°á»£c init zeros Ä‘á»ƒ training á»•n Ä‘á»‹nh |

### 3. **AdaRMSNormAdapter** (Adaptive Layer Norm)
```
instruction_emb [B, 1024]
       â†“
Linear(1024 â†’ hidden) â†’ SiLU â†’ Linear(hidden â†’ 2048)
       â†“
Split â†’ Î³_ada [B, 1, 1024], Î²_ada [B, 1, 1024]
       â†“
output = RMSNorm(x) Ã— (1 + Î³_ada) + Î²_ada
```

### 4. **Speaker Embedding Dropout** (Training trick)
- Trong training: **20% chance** speaker_emb bá»‹ zero-out
- Má»¥c Ä‘Ã­ch: Ã‰p model há»c tá»« instruction text thay vÃ¬ chá»‰ dá»±a vÃ o speaker embedding
- KhÃ´ng Ã¡p dá»¥ng khi inference

### 5. **Trainable vs Frozen Parameters**
| Module | Status |
|--------|--------|
| T5 Encoder | â„ï¸ FROZEN |
| Attention Pooling Query | ğŸ”¥ Trainable |
| Attention Pooling MHA | ğŸ”¥ Trainable |
| Linear Projection | ğŸ”¥ Trainable |
| Original LLaMA weights | â„ï¸ FROZEN (backbone) |
| AdaRMSNormAdapter (Ã—60) | ğŸ”¥ Trainable |
| Voice Encoder | â„ï¸ FROZEN |
| S3Gen | â„ï¸ FROZEN |

