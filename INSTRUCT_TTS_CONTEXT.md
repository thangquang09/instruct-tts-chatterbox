# Instruct-TTS Chatterbox - Implementation Context

> **Version:** 2.0 (Dec 2025) - Updated with bug fixes and lessons learned

---

Ta sáº½ sá»­ dá»¥ng uv run trong project nÃ y.

## 1. Tá»•ng quan Kiáº¿n trÃºc

Dá»± Ã¡n má»Ÿ rá»™ng **Chatterbox T3** Ä‘á»ƒ Ä‘iá»u khiá»ƒn giá»ng nÃ³i báº±ng **Instruction Text** thay vÃ¬ chá»‰ dÃ¹ng Reference Audio.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INSTRUCT-TTS PIPELINE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  Instruction Text â”€â”€â–º [T5 Encoder] â”€â”€â–º [Attn Pooling] â”€â”€â–º (1024)     â”‚
â”‚                         (frozen)        (trainable)                   â”‚
â”‚                                              â”‚                        â”‚
â”‚                                              â–¼                        â”‚
â”‚  Text + Speech â”€â”€â–º [CustomLlamaModel] â—„â”€â”€ AdaRMSNorm Modulation      â”‚
â”‚     Tokens              (trainable)                                   â”‚
â”‚                              â”‚                                        â”‚
â”‚                              â–¼                                        â”‚
â”‚                      [Text + Speech Heads] â”€â”€â–º Logits                â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ThÃ nh pháº§n chÃ­nh

| Component | File | Trainable | MÃ´ táº£ |
|-----------|------|-----------|-------|
| **InstructionEncoder** | `modules/instruction_encoder.py` | Partial | T5 frozen, Attn+Proj trainable |
| **CustomLlamaModel** | `modeling_llama_adapter.py` | âœ… Yes | LlamaModel + AdaRMSNorm adapters |
| **AdaRMSNormAdapter** | `modeling_llama_adapter.py` | âœ… Yes | Modulates hidden states vá»›i instruction |
| **T3** | `t3.py` | âœ… Yes | Main model, tÃ­ch há»£p táº¥t cáº£ |

---

## 2. Implementation (Verified & Fixed Code)

### 2.1. InstructionEncoder

**File:** `src/chatterbox/models/t3/modules/instruction_encoder.py`

**âš ï¸ CRITICAL:** Module nÃ y pháº£i cháº¡y trong **FP32** Ä‘á»ƒ trÃ¡nh NaN vá»›i FP16 training.

```python
class InstructionEncoder(nn.Module):
    def __init__(self, model_name="google/flan-t5-large"):
        super().__init__()
        self.t5 = T5EncoderModel.from_pretrained(model_name)
        
        # Clone tied weights Ä‘á»ƒ fix Safetensors saving issue
        if hasattr(self.t5, "shared") and hasattr(self.t5.encoder, "embed_tokens"):
            if self.t5.shared.weight.data_ptr() == self.t5.encoder.embed_tokens.weight.data_ptr():
                self.t5.encoder.embed_tokens.weight.data = self.t5.encoder.embed_tokens.weight.data.clone()
        
        # Freeze T5
        for param in self.t5.parameters():
            param.requires_grad = False
            
        self.hidden_size = self.t5.config.d_model  # 1024 for flan-t5-large
        
        # Trainable components (NO projection - output is hidden_size=1024)
        self.query = nn.Parameter(torch.randn(1, 1, self.hidden_size) * 0.02)
        self.attn = nn.MultiheadAttention(embed_dim=self.hidden_size, num_heads=8, batch_first=True)
        
        # Xavier init for stability
        nn.init.xavier_uniform_(self.attn.in_proj_weight)
        nn.init.xavier_uniform_(self.attn.out_proj.weight)
        nn.init.xavier_uniform_(self.proj.weight)

    def forward(self, input_ids, attention_mask):
        # âš ï¸ CRITICAL: Disable autocast to avoid FP16 NaN issues
        with torch.amp.autocast('cuda', enabled=False):
            with torch.no_grad():
                outputs = self.t5(input_ids=input_ids, attention_mask=attention_mask)
                encoder_hidden_states = outputs.last_hidden_state.detach().float()
            
            batch_size = input_ids.shape[0]
            query = self.query.float().expand(batch_size, -1, -1)
            key_padding_mask = (attention_mask == 0).to(device=encoder_hidden_states.device)
            
            # Handle all-masked case
            all_masked = key_padding_mask.all(dim=1)
            if all_masked.any():
                key_padding_mask[all_masked] = False
            
            style_emb, _ = self.attn(query, encoder_hidden_states, encoder_hidden_states, 
                                     key_padding_mask=key_padding_mask)
            style_vector = F.linear(style_emb, self.proj.weight.float(), 
                                   self.proj.bias.float()).squeeze(1)
        
        return style_vector
```

### 2.2. AdaRMSNormAdapter

**File:** `src/chatterbox/models/t3/modeling_llama_adapter.py`

```python
class AdaRMSNormAdapter(nn.Module):
    """Modulates RMSNorm output based on instruction embedding."""
    
    def __init__(self, hidden_size: int, instruction_dim: int):
        super().__init__()
        self.adapter = nn.Sequential(
            nn.Linear(instruction_dim, hidden_size),
            nn.SiLU(),
            nn.Linear(hidden_size, hidden_size * 2)
        )
        # Zero-init for residual learning
        nn.init.zeros_(self.adapter[-1].weight)
        nn.init.zeros_(self.adapter[-1].bias)
        
    def forward(self, x, instruction_emb, original_norm_layer):
        normed_x = original_norm_layer(x)
        
        # âš ï¸ Handle None instruction_emb (important for inference without instruction)
        if instruction_emb is None:
            return normed_x
        
        style_params = self.adapter(instruction_emb).unsqueeze(1)
        gamma, beta = style_params.chunk(2, dim=-1)
        return normed_x * (1 + gamma) + beta
```

### 2.3. T3 Integration (Key Parts)

**File:** `src/chatterbox/models/t3/t3.py`

```python
class T3(nn.Module):
    def __init__(self, hp=T3Config()):
        # ...
        adapter_config = {"instruction_dim": 1024}
        self.tfmr = CustomLlamaModel(self.cfg, adapter_config)
        
        # InstructionEncoder - T5 frozen inside, adapters trainable
        # âš ï¸ No output_dim argument - output size is hidden_size (1024)
        self.instr_encoder = InstructionEncoder("google/flan-t5-large")
        # âš ï¸ Do NOT freeze entire instr_encoder here - let finetune script control it

    def forward(self, ..., instruction_input_ids=None, instruction_attention_mask=None):
        # ...
        instruction_emb = None
        if hasattr(self, "instr_encoder") and instruction_input_ids is not None:
            # âš ï¸ Ensure correct device
            instruction_input_ids = instruction_input_ids.to(self.device)
            if instruction_attention_mask is not None:
                instruction_attention_mask = instruction_attention_mask.to(self.device)
            
            instruction_emb = self.instr_encoder(instruction_input_ids, instruction_attention_mask)
            # âš ï¸ Align dtype with embeddings
            instruction_emb = instruction_emb.to(dtype=embeds.dtype)
        
        tfmr_out = self.tfmr.forward(..., instruction_emb=instruction_emb)
```

---

## 3. Training Script - Stage 2: T3 Finetuning vá»›i Mapper

**File:** `src/finetune_t3.py`

### 3.1. Freezing Strategy (Stage 2)

```python
# 1. Freeze Voice Encoder vÃ  S3Gen
for param in model.ve.parameters():
    param.requires_grad = False
for param in model.s3gen.parameters():
    param.requires_grad = False

# 2. Load Mapper vÃ  InstructionEncoder tá»« checkpoint
mapper_ckpt = torch.load("./checkpoints/mapper_phase2/best_model.pt")
instruction_mapper = InstructionMapper()
instruction_mapper.load_state_dict(mapper_ckpt["mapper"])
instruction_mapper.eval()  # Freeze

# Load InstructionEncoder weights
model.t3.instr_encoder.load_state_dict(mapper_ckpt["encoder"], strict=False)

# 3. Unfreeze T3 (main model)
for param in model.t3.parameters():
    param.requires_grad = True

# 4. âš ï¸ Freeze ENTIRE InstructionEncoder (NOT just T5)
for param in model.t3.instr_encoder.parameters():
    param.requires_grad = False
```

**Trainable Components (Stage 2):**
| Component | Trainable | Ghi chÃº |
|-----------|-----------|--------|
| LlamaModel | âœ… Yes | Main backbone (~520M) |
| AdaRMSNorm Adapters | âœ… Yes | 30 layers Ã— 2 adapters |
| Text/Speech Embeddings | âœ… Yes | Input embeddings |
| Text/Speech Heads | âœ… Yes | Output heads |
| InstructionEncoder | â„ï¸ **FROZEN** | Weights from Mapper |
| InstructionMapper | â„ï¸ **FROZEN** | For SpkEmb prediction |
| VoiceEncoder, S3Gen | â„ï¸ Frozen | Original pretrained |

### 3.2. 50/50 Mixing Strategy

Má»—i batch Ä‘Æ°á»£c chia random 50/50:

```python
if self.training and self.mapper is not None:
    use_instruction = torch.rand(B) < 0.5  # Per-sample random
    
    # Instruction-Only samples:
    # - SpkEmb = Mapper(InstructionEncoder(instruction))
    # - PromptTokens = zeros (no reference audio)
    
    # Audio-Only samples:
    # - SpkEmb = Ground Truth from VoiceEncoder
    # - PromptTokens = Ground Truth
    # - Instruction attention_mask = 0 (masked)
```

### 3.3. Loss Handling Edge Case

```python
def loss(self, logits_for_speech, labels_speech, ...):
    # âš ï¸ Handle all-masked labels (would cause NaN)
    valid_speech_tokens = (labels_speech != IGNORE_ID).sum()
    if valid_speech_tokens == 0:
        loss_speech = torch.tensor(0.0, device=device, requires_grad=self.training)
    else:
        loss_speech = F.cross_entropy(logits_for_speech.transpose(1, 2), 
                                      labels_speech, ignore_index=IGNORE_ID)
```

## 4. Training Command (Stage 2)

```bash
bash t3_finetune.sh  # Full training
bash t3_finetune.sh --mock  # Quick test (20 steps)
```

**Script arguments:**
```bash
CUDA_VISIBLE_DEVICES=1 python src/finetune_t3.py \
    --do_train \
    --mapper_ckpt_path "./checkpoints/mapper_phase2/best_model.pt" \
    --instruction_dropout_prob 0.5 \  # 50/50 mixing
    --metadata_file "captts_sft_expresso.txt" \
    --learning_rate 1e-5 \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --freeze_voice_encoder True \
    --freeze_s3gen True
```

---

## 5. Data Format

**Metadata file** (pipe-separated):
```
path/to/audio1.wav|Ná»™i dung vÄƒn báº£n 1|HÃ£y nÃ³i vá»›i giá»ng vui váº» vÃ  hÃ o há»©ng
path/to/audio2.wav|Ná»™i dung vÄƒn báº£n 2|NÃ³i giá»ng buá»“n, cháº­m rÃ£i
```

---

## 6. âš ï¸ Known Issues & Fixes

### Issue 1: NaN Loss vá»›i FP16

**NguyÃªn nhÃ¢n:** T5 + MultiheadAttention khÃ´ng stable vá»›i FP16

**Fix:** Wrap `InstructionEncoder.forward()` trong `torch.amp.autocast('cuda', enabled=False)`

### Issue 2: Safetensors Shared Memory Error

**NguyÃªn nhÃ¢n:** T5 cÃ³ tied weights (`shared.weight` == `embed_tokens.weight`)

**Fix:** Clone tensors trÆ°á»›c khi save, hoáº·c dÃ¹ng `--save_safetensors False`

### Issue 3: All-Masked Labels â†’ NaN Loss

**NguyÃªn nhÃ¢n:** `F.cross_entropy` vá»›i táº¥t cáº£ labels = `ignore_index` â†’ NaN

**Fix:** Check `valid_tokens == 0` vÃ  return `0.0` thay vÃ¬ compute loss

### Issue 4: Device Mismatch

**NguyÃªn nhÃ¢n:** Instruction tensors khÃ´ng Ä‘Æ°á»£c move sang GPU

**Fix:** Explicit `.to(self.device)` trong `T3.forward()`

---

## 7. Debugging Checklist

Khi gáº·p NaN loss:

```
â–¡ 1. Táº¯t --fp16, cháº¡y láº¡i â†’ Náº¿u háº¿t NaN â†’ FP16 lÃ  nguyÃªn nhÃ¢n
â–¡ 2. ThÃªm debug logging: torch.isnan(tensor).any()
â–¡ 3. Check trainable params: sum(p.requires_grad for p in model.parameters())
â–¡ 4. Verify devices: tensor.device cho má»i inputs
â–¡ 5. Check edge cases: all labels masked?
```

Chi tiáº¿t: Xem `Troubleshooting_Tips.md`

---

*Last Updated: December 2025*

---

## 8. Instruction Mapper (Stage 1 Pre-training)

### 8.1. Tá»•ng quan 2-Stage Training Strategy

Äá»ƒ giáº£m sá»± phá»¥ thuá»™c vÃ o Reference Audio, ta cáº§n train má»™t **Mapper** Ä‘á»ƒ Ã¡nh xáº¡ tá»« **Instruction Text** sang **Speaker Embedding** vÃ  **X-Vector**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    2-STAGE TRAINING STRATEGY                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  STAGE 1: Train Instruction Mapper (RiÃªng biá»‡t)                          â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                         â”‚
â”‚                                                                          â”‚
â”‚  Instruction â”€â”€â–º [T5] â”€â”€â–º [Query+Attn] â”€â”€â–º style_emb â”€â”€â–º [Mapper]        â”‚
â”‚     Text       FREEZE      TRAIN              â”‚           TRAIN          â”‚
â”‚                                               â”‚              â”‚           â”‚
â”‚                                               â”‚         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”      â”‚
â”‚                                               â”‚         â–¼         â–¼      â”‚
â”‚                                               â”‚     SpkEmb     X-Vec     â”‚
â”‚                                               â”‚      (256)     (192)     â”‚
â”‚                                               â”‚         â”‚         â”‚      â”‚
â”‚                Audio (GT) â”€â”€â–º [VoiceEnc] â”€â”€â”€â”€â–ºâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”‚
â”‚                            â”€â”€â–º [CAM++]  â”€â”€â”€â”€â”€â–ºâ”‚   Flow Matching   â”‚      â”‚
â”‚                                               â”‚   Loss (MSE)      â”‚      â”‚
â”‚                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                          â”‚
â”‚  STAGE 2: Finetune T3 (Sá»­ dá»¥ng Mapper Ä‘Ã£ train)                          â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                          â”‚
â”‚                                                                          â”‚
â”‚  Instruction â”€â”€â–º [T5+Query+Attn] â”€â”€â–º style_emb â”€â”€â–º [proj] â”€â”€â–º T3         â”‚
â”‚                       FREEZE              â”‚        TRAIN      TRAIN      â”‚
â”‚                                           â–¼                              â”‚
â”‚                                    [Mapper Heads] (FREEZE)               â”‚
â”‚                                     â†’ SpkEmb, X-Vec â†’ S3Gen              â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2. InstructionMapper Architecture (Flow Matching)

**File:** `src/chatterbox/models/t3/modules/instruction_mapper.py`

Sá»­ dá»¥ng kiáº¿n trÃºc **Conditional Flow Matching** vá»›i **AdaLN (Adaptive Layer Normalization)**.

```python
class InstructionMapper(nn.Module):
    """
    Flow Matching Backbone.
    Input: style_emb (1024) tá»« InstructionEncoder
    Output: velocity prediction (512) cho ODE sampling
    Final: SpkEmb (256), X-Vec (192)
    """
    def __init__(self, input_dim=1024, internal_dim=512, 
                 spk_emb_dim=256, xvec_dim=192, depth=6):
        # Time embedding (Sinusoidal)
        self.time_mlp = SinusoidalPosEmb + MLP
        
        # Condition projection
        self.cond_proj = Linear(1024 â†’ 512)
        
        # ResNet-MLP vá»›i AdaLN blocks
        self.blocks = [AdaLNBlock(512, 512) for _ in range(6)]
        
        # Output heads
        self.head_spk = Linear(512 â†’ 256)
        self.head_xvec = Linear(512 â†’ 192)
    
    def forward(self, x, t, style_emb):
        """Train mode: Predict velocity v."""
        return v_pred  # [B, 512]
    
    def inference(self, style_emb, num_steps=10):
        """Inference mode: ODE sampling â†’ (spk_emb, x_vector)"""
        return spk_emb, x_vector
```

### 8.3. InstructionEncoder (Refactored)

**File:** `src/chatterbox/models/t3/modules/instruction_encoder.py`

ÄÃ£ refactor Ä‘á»ƒ loáº¡i bá» projection layer thá»«a. Chá»‰ output `style_emb` size 1024.

```python
def forward(self, input_ids, attention_mask):
    # ... T5 + Attention Pooling ...
    style_emb, _ = self.attn(query, encoder_hidden_states, ...)  # [B, 1, 1024]
    return style_emb.squeeze(1)  # [B, 1024] - Trunk output
```

### 8.4. Two-Phase Training Scripts (Stage 1)

Training Ä‘Æ°á»£c chia thÃ nh 2 script riÃªng biá»‡t Ä‘á»ƒ Ä‘áº£m báº£o á»•n Ä‘á»‹nh tá»‘i Ä‘a:

**Phase 1: `run_phase1.sh` (GT Reconstruction)**
- **Má»¥c tiÃªu:** Train stable heads & backbone projection.
- **Input Reconstruction:** DÃ¹ng Ground Truth `x_1` (concat GT SpkEmb + XVec).
- **Flag:** Default (khÃ´ng dÃ¹ng `--use_predicted_recon`).
- **Early Stopping:** Dá»±a trÃªn Avg Cosine Similarity.

**Phase 2: `run_phase2.sh` (Predicted Reconstruction)**
- **Má»¥c tiÃªu:** End-to-end optimization, giáº£m gap giá»¯a train/inference.
- **Input Reconstruction:** DÃ¹ng `x_1_pred = x_t + v_pred * (1-t)`.
- **Resume:** Load checkpoint tá»« Phase 1.
- **Flag:** `--use_predicted_recon --reset_best_cos`.
- **LR:** Tháº¥p hÆ¡n (5e-5) Ä‘á»ƒ fine-tune.

### 8.5. Training Components & Fixes

| Component | Trainable | Ghi chÃº |`
|-----------|-----------|---------|
| T5 Encoder | â„ï¸ Frozen | Pretrained knowledge |
| InstructionEncoder | âœ… Train | Query + Attn (No Proj) |
| InstructionMapper | âœ… Train | AdaLN blocks, Heads |
| **Target Construction** | â„ï¸ **FIXED** | `x_1` = Concat[GT_Spk, GT_XVec, Pad] (No LatentProjector!) |

> âš ï¸ **CRITICAL FIX:** Target cá»§a Flow Matching pháº£i lÃ  **FIXED** (concat GT embeddings). KhÃ´ng dÃ¹ng trainable projector Ä‘á»ƒ táº¡o target, trÃ¡nh hiá»‡n tÆ°á»£ng collapse loss.

### 8.6. Training Results (300k Data - Dec 2025)

Káº¿t quáº£ training Phase 1 trÃªn dataset 300k samples:

| Epoch | Flow Loss | Recon Loss | SpkCos | XVecCos | Avg Cos |
|-------|-----------|------------|--------|---------|---------|
| 1 | 0.496 | 0.0005 | **0.237** | **0.111** | 0.174 |
| 2 | 0.058 | 0.0000 | **0.789** | **0.515** | **0.652** |

**Nháº­n xÃ©t:** Model há»™i tá»¥ cá»±c nhanh vá»›i dá»¯ liá»‡u lá»›n. SpkCos Ä‘áº¡t ~0.79 chá»‰ sau 2 epochs.

### 8.7. Loading cho Stage 2

Sau khi xong Phase 2, load weights Ä‘á»ƒ fine-tune T3:

```python
# 1. Load Mapper weights
checkpoint = torch.load("./checkpoints/mapper_phase2/best_model.pt")

# 2. Load InstructionEncoder (Query + Attn)
model.t3.instr_encoder.load_state_dict(checkpoint['encoder'], strict=False)

# 3. Load InstructionMapper (cho S3Gen inference)
model.instruction_mapper.load_state_dict(checkpoint['mapper'])
model.instruction_mapper.eval()  # Freeze mapper in Stage 2
```

### 8.8. Tensor Shapes

| Tensor | Shape | MÃ´ táº£ |
|--------|-------|-------|
| `style_emb` | [B, 1024] | Trunk output (T5 + Attn Pooling) |
| `x_1` | [B, 512] | Fixed target = [SpkEmb, XVec, Pad] |
| `v_pred` | [B, 512] | Predicted velocity |
| `spk_emb` | [B, 256] | Speaker Embedding (cho T3 T3Cond) |
| `x_vector` | [B, 192] | CAM++ X-Vector (cho S3Gen) |

## 9. Weight Loading Strategy

After the user's update to finetune_t3.py , the checkpoint structure is:

![alt text](image.png)


```
Total items found: 305267
Sampling 10000 items for analysis...

############################################################
NORM STATISTICS REPORT
############################################################

==================== Speaker Embedding (VoiceEncoder) ====================
Sample Count: 10000
Mean Norm:    1.0000
Std Dev:      0.0000
Min Norm:     1.0000
Max Norm:     1.0000
--------------------------------------------------
âœ… OK: Speaker Embedding (VoiceEncoder) appears to be already normalized (Unit Vector).

==================== X-Vector (CAMPPlus) ====================
Sample Count: 10000
Mean Norm:    14.0775
Std Dev:      1.4688
Min Norm:     9.5379
Max Norm:     21.9082
--------------------------------------------------
âš ï¸  WARNING: X-Vector (CAMPPlus) has very large norm (~14.08).
   -> Recommendation: Apply Pre-Normalization or L2 Normalize in code.

ğŸ“Š COMPARISON:
Ratio (X-Vec / Spk): 14.08
âŒ IMBALANCE DETECTED: One vector type is significantly larger than the other.
   -> This causes MSE Loss to focus only on the larger vector.
   -> ACTION: Normalize both inputs to unit length (norm=1) before training.

```
