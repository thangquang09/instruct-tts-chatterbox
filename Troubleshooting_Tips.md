# Troubleshooting Tips: Deep Learning Training Issues

> **TÃ i liá»‡u nÃ y Ä‘Æ°á»£c táº¡o tá»« kinh nghiá»‡m debug issue NaN loss trong dá»± Ã¡n Instruct-TTS-Chatterbox (Dec 2025)**

---

## ğŸ“‹ Table of Contents

1. [Quick Diagnosis Checklist](#quick-diagnosis-checklist)
2. [NaN/Inf Loss Debugging Playbook](#naninf-loss-debugging-playbook)
3. [FP16/Mixed Precision Issues](#fp16mixed-precision-issues)
4. [Common Root Causes](#common-root-causes)
5. [Debug Logging Templates](#debug-logging-templates)
6. [Prevention Strategies](#prevention-strategies)

---

## ğŸ¯ Quick Diagnosis Checklist

**Khi gáº·p NaN/Inf loss, hÃ£y kiá»ƒm tra theo thá»© tá»± Æ°u tiÃªn sau:**

```
â–¡ 1. [5 min] Táº¯t FP16, cháº¡y láº¡i vá»›i FP32 â†’ NaN háº¿t? â†’ FP16 lÃ  nguyÃªn nhÃ¢n
â–¡ 2. [2 min] Kiá»ƒm tra learning rate (thá»­ giáº£m 10x)
â–¡ 3. [5 min] In ra input tensors: cÃ³ NaN khÃ´ng? Device Ä‘Ãºng chÆ°a?
â–¡ 4. [10 min] ThÃªm debug logging táº¡i Tá»ªNG module â†’ xÃ¡c Ä‘á»‹nh NaN xuáº¥t hiá»‡n á»Ÿ Ä‘Ã¢u
â–¡ 5. [5 min] Kiá»ƒm tra loss function vá»›i edge cases (empty labels, all masked)
â–¡ 6. [5 min] Verify trainable parameters (param.requires_grad == True?)
```

---

## ğŸ”¥ NaN/Inf Loss Debugging Playbook

### Step 1: Binary Search Ä‘á»ƒ xÃ¡c Ä‘á»‹nh nguá»“n NaN

**NguyÃªn táº¯c vÃ ng: Äá»«ng Ä‘oÃ¡n, hÃ£y Ä‘o!**

```python
# ThÃªm vÃ o forward() cá»§a tá»«ng module
def debug_tensor(name, tensor, step=None):
    """In debug info cho tensor"""
    if tensor is None:
        print(f"[DEBUG] {name}: None")
        return
    
    has_nan = torch.isnan(tensor).any().item()
    has_inf = torch.isinf(tensor).any().item()
    
    status = "âœ“" if not (has_nan or has_inf) else "âš ï¸ NaN!" if has_nan else "âš ï¸ Inf!"
    
    print(f"[DEBUG] {name}: shape={tensor.shape}, dtype={tensor.dtype}, "
          f"device={tensor.device}, {status}")
    
    if has_nan or has_inf:
        print(f"  â†’ min={tensor[~torch.isnan(tensor)].min().item():.4f}, "
              f"max={tensor[~torch.isnan(tensor)].max().item():.4f}")
```

### Step 2: CÃ´ láº­p module cÃ³ váº¥n Ä‘á»

```python
# Trong main forward pass, thÃªm checkpoints
def forward(self, ...):
    # Checkpoint 1: Input
    debug_tensor("input_embeds", embeds)
    
    # Checkpoint 2: Sau má»—i sub-module
    instruction_emb = self.instr_encoder(...)
    debug_tensor("instruction_emb", instruction_emb)  # â† NaN xuáº¥t hiá»‡n á»Ÿ Ä‘Ã¢y!
    
    # Checkpoint 3: Hidden states
    hidden_states = self.backbone(...)
    debug_tensor("hidden_states", hidden_states)
    
    # Checkpoint 4: Logits
    logits = self.output_head(hidden_states)
    debug_tensor("logits", logits)
```

### Step 3: Drill down vÃ o module cÃ³ váº¥n Ä‘á»

```python
# Khi Ä‘Ã£ biáº¿t module nÃ o cÃ³ NaN, thÃªm debug chi tiáº¿t hÆ¡n
class InstructionEncoder(nn.Module):
    def forward(self, input_ids, attention_mask=None):
        # Debug tá»«ng bÆ°á»›c trong module
        debug_tensor("1. input_ids", input_ids)
        
        outputs = self.t5(input_ids, attention_mask)
        debug_tensor("2. t5_output", outputs.last_hidden_state)  # â† NaN á»Ÿ Ä‘Ã¢y!
        
        style_emb, _ = self.attn(query, outputs.last_hidden_state, ...)
        debug_tensor("3. after_attention", style_emb)
        
        result = self.proj(style_emb)
        debug_tensor("4. final_output", result)
        
        return result
```

---

## âš¡ FP16/Mixed Precision Issues

### CÃ¡c module HAY GÃ‚Y NaN vá»›i FP16

| Module | NguyÃªn nhÃ¢n | Giáº£i phÃ¡p |
|--------|-------------|-----------|
| `nn.MultiheadAttention` | Softmax overflow vá»›i large values | Force FP32 |
| `LayerNorm` / `RMSNorm` | Division by small variance | Force FP32 hoáº·c eps lá»›n hÆ¡n |
| Large pretrained models (T5, BERT) | Internal ops khÃ´ng stable vá»›i FP16 | Wrap trong `autocast(enabled=False)` |
| Cross-entropy loss | Log cá»§a values gáº§n 0 | Clamp logits hoáº·c FP32 loss |

### Pattern: Force FP32 cho module khÃ´ng stable

```python
def forward(self, input_ids, attention_mask=None):
    # Force FP32 cho toÃ n bá»™ module
    with torch.amp.autocast('cuda', enabled=False):
        # Cast inputs vá» FP32
        query = self.query.float()
        
        # Forward pass trong FP32
        outputs = self.encoder(input_ids, attention_mask)
        hidden_states = outputs.last_hidden_state.float()
        
        # Attention trong FP32
        attn_output, _ = self.attn(query, hidden_states, hidden_states)
        
        result = self.proj(attn_output)
    
    return result  # CÃ³ thá»ƒ cast láº¡i FP16 náº¿u cáº§n
```

### Quick Test: FP16 cÃ³ pháº£i nguyÃªn nhÃ¢n khÃ´ng?

```bash
# Test 1: Cháº¡y vá»›i FP32
python train.py --fp16 false

# Test 2: Náº¿u FP32 work â†’ FP16 lÃ  váº¥n Ä‘á»
# TÃ¬m module nÃ o cáº§n force FP32
```

---

## ğŸ¯ Common Root Causes

### 1. Incorrect Parameter Freezing

**Symptom**: `grad_norm=nan` hoáº·c loss khÃ´ng giáº£m

**Check**:
```python
# Kiá»ƒm tra trainable parameters
trainable = [n for n, p in model.named_parameters() if p.requires_grad]
frozen = [n for n, p in model.named_parameters() if not p.requires_grad]

print(f"Trainable: {len(trainable)}")
print(f"Frozen: {len(frozen)}")

# In chi tiáº¿t náº¿u nghi ngá»
for name in trainable[:10]:
    print(f"  âœ“ {name}")
```

**Common Mistake**:
```python
# âŒ SAI: Freeze cáº£ module, bao gá»“m adapter trainable
for param in model.encoder.parameters():
    param.requires_grad = False

# âœ“ ÄÃšNG: Chá»‰ freeze pháº§n cáº§n thiáº¿t
for param in model.encoder.pretrained_part.parameters():
    param.requires_grad = False
# Giá»¯ adapter trainable
for param in model.encoder.adapter.parameters():
    param.requires_grad = True
```

### 2. Device/Dtype Mismatch

**Symptom**: RuntimeError hoáº·c silent NaN

**Check**:
```python
def check_model_devices(model):
    devices = set()
    dtypes = set()
    for name, param in model.named_parameters():
        devices.add(str(param.device))
        dtypes.add(str(param.dtype))
    print(f"Devices: {devices}")
    print(f"Dtypes: {dtypes}")
```

**Fix Pattern**:
```python
def forward(self, input_ids, ...):
    # Explicit device alignment
    input_ids = input_ids.to(self.device)
    
    # Explicit dtype alignment
    embeds = self.embed(input_ids)
    condition = condition.to(dtype=embeds.dtype, device=embeds.device)
```

### 3. Edge Cases trong Loss Computation

**Symptom**: NaN loss vá»›i má»™t sá»‘ batches

**Common Cases**:
```python
# âŒ All labels masked â†’ NaN
loss = F.cross_entropy(logits, labels, ignore_index=-100)
# Náº¿u táº¥t cáº£ labels == -100 â†’ loss = nan

# âœ“ Handle edge case
valid_tokens = (labels != -100).sum()
if valid_tokens == 0:
    loss = torch.tensor(0.0, device=logits.device, requires_grad=True)
else:
    loss = F.cross_entropy(logits, labels, ignore_index=-100)
```

### 4. Uninitialized/Bad Weight Initialization

**Symptom**: NaN ngay tá»« step Ä‘áº§u tiÃªn

**Fix**:
```python
# Proper initialization cho attention
nn.init.xavier_uniform_(self.attn.in_proj_weight)
nn.init.xavier_uniform_(self.attn.out_proj.weight)
nn.init.zeros_(self.attn.out_proj.bias)

# Scaled initialization cho learnable parameters
self.query = nn.Parameter(torch.randn(1, 1, hidden_size) * 0.02)
```

---

## ğŸ“ Debug Logging Templates

### Template 1: Training Loop Debug

```python
# Trong custom training step
def training_step(self, model, inputs):
    # Log inputs
    logger.info(f"[Step {self.state.global_step}] Input shapes:")
    for k, v in inputs.items():
        if isinstance(v, torch.Tensor):
            has_nan = torch.isnan(v).any().item()
            logger.info(f"  {k}: {v.shape}, nan={has_nan}")
    
    # Forward
    outputs = model(**inputs)
    
    # Log outputs
    loss = outputs.loss
    logger.info(f"[Step {self.state.global_step}] Loss: {loss.item():.4f}")
    
    if torch.isnan(loss):
        logger.error("NaN loss detected! Dumping debug info...")
        self._dump_debug_info(inputs, outputs)
        raise ValueError("NaN loss")
    
    return loss
```

### Template 2: Module-level Debug

```python
class MyModule(nn.Module):
    def __init__(self, debug=False):
        super().__init__()
        self.debug = debug
    
    def forward(self, x):
        if self.debug:
            self._check_tensor("input", x)
        
        # ... operations ...
        
        if self.debug:
            self._check_tensor("output", output)
        
        return output
    
    def _check_tensor(self, name, tensor):
        if tensor is None:
            print(f"[{self.__class__.__name__}] {name}: None")
            return
        
        nan_count = torch.isnan(tensor).sum().item()
        inf_count = torch.isinf(tensor).sum().item()
        
        if nan_count > 0 or inf_count > 0:
            print(f"[{self.__class__.__name__}] âš ï¸ {name}: "
                  f"NaN={nan_count}, Inf={inf_count}, shape={tensor.shape}")
```

---

## ğŸ›¡ï¸ Prevention Strategies

### 1. Pre-training Sanity Checks

```python
def sanity_check_before_training(model, dataloader, device):
    """Cháº¡y trÆ°á»›c khi training Ä‘á»ƒ phÃ¡t hiá»‡n issues sá»›m"""
    
    print("=" * 50)
    print("PRE-TRAINING SANITY CHECK")
    print("=" * 50)
    
    # Check 1: Model devices
    print("\n1. Checking model devices...")
    devices = {p.device for p in model.parameters()}
    print(f"   Model on devices: {devices}")
    
    # Check 2: Trainable params
    print("\n2. Checking trainable parameters...")
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    print(f"   Trainable: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)")
    
    # Check 3: Forward pass
    print("\n3. Testing forward pass (1 batch, no grad)...")
    model.eval()
    batch = next(iter(dataloader))
    batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
             for k, v in batch.items()}
    
    with torch.no_grad():
        try:
            outputs = model(**batch)
            loss = outputs.loss if hasattr(outputs, 'loss') else outputs
            print(f"   Forward pass OK, loss = {loss.item():.4f}")
            
            if torch.isnan(loss):
                print("   âš ï¸ WARNING: Loss is NaN!")
        except Exception as e:
            print(f"   âŒ Forward pass FAILED: {e}")
            return False
    
    # Check 4: Backward pass
    print("\n4. Testing backward pass...")
    model.train()
    try:
        outputs = model(**batch)
        loss = outputs.loss if hasattr(outputs, 'loss') else outputs
        loss.backward()
        
        # Check gradients
        nan_grads = sum(1 for p in model.parameters() 
                       if p.grad is not None and torch.isnan(p.grad).any())
        print(f"   Backward pass OK, NaN gradients: {nan_grads}")
        
        model.zero_grad()
    except Exception as e:
        print(f"   âŒ Backward pass FAILED: {e}")
        return False
    
    print("\n" + "=" * 50)
    print("SANITY CHECK PASSED âœ“")
    print("=" * 50)
    return True
```

### 2. Gradient Monitoring

```python
# ThÃªm vÃ o training loop
def check_gradients(model, step):
    """Monitor gradients for anomalies"""
    total_norm = 0
    nan_params = []
    inf_params = []
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            param_norm = param.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
            
            if torch.isnan(param.grad).any():
                nan_params.append(name)
            if torch.isinf(param.grad).any():
                inf_params.append(name)
    
    total_norm = total_norm ** 0.5
    
    if nan_params:
        print(f"[Step {step}] âš ï¸ NaN gradients in: {nan_params[:5]}...")
    if inf_params:
        print(f"[Step {step}] âš ï¸ Inf gradients in: {inf_params[:5]}...")
    if total_norm > 100:
        print(f"[Step {step}] âš ï¸ Large grad norm: {total_norm:.2f}")
    
    return total_norm
```

### 3. FP16 Compatibility Test

```python
def test_fp16_compatibility(model, sample_input, device):
    """Test if model works with FP16"""
    
    print("Testing FP16 compatibility...")
    model = model.to(device)
    
    # Test FP32
    model.float()
    with torch.no_grad():
        out_fp32 = model(**sample_input)
        loss_fp32 = out_fp32.loss.item()
    print(f"  FP32 loss: {loss_fp32:.4f}")
    
    # Test FP16
    model.half()
    sample_input_fp16 = {k: v.half() if v.dtype == torch.float32 else v 
                         for k, v in sample_input.items()}
    
    with torch.no_grad():
        try:
            out_fp16 = model(**sample_input_fp16)
            loss_fp16 = out_fp16.loss.item()
            print(f"  FP16 loss: {loss_fp16:.4f}")
            
            if torch.isnan(out_fp16.loss):
                print("  âš ï¸ FP16 produces NaN! Use FP32 or fix unstable modules.")
                return False
        except Exception as e:
            print(f"  âŒ FP16 failed: {e}")
            return False
    
    # Test autocast
    model.float()
    with torch.amp.autocast('cuda'):
        out_autocast = model(**sample_input)
        loss_autocast = out_autocast.loss.item()
    print(f"  Autocast loss: {loss_autocast:.4f}")
    
    if torch.isnan(out_autocast.loss):
        print("  âš ï¸ Autocast produces NaN!")
        return False
    
    print("  âœ“ FP16 compatibility OK")
    return True
```

---

## ğŸ“š Lessons Learned (Case Study: Instruct-TTS)

### Váº¥n Ä‘á» gáº·p pháº£i
- **Triá»‡u chá»©ng**: `loss: nan`, `grad_norm: nan` khi fine-tune model vá»›i instruction encoder
- **Thá»i gian debug**: ~2 giá» (quÃ¡ lÃ¢u!)

### Root Causes tÃ¬m Ä‘Æ°á»£c
1. **FP16 + T5 + MultiheadAttention** = Numerical instability
2. **Incorrect freezing**: Freeze cáº£ trainable adapter components
3. **Device mismatch**: Instruction tensors khÃ´ng Ä‘Æ°á»£c move sang GPU
4. **Edge case**: All-masked labels gÃ¢y NaN trong cross-entropy

### Sai láº§m trong quÃ¡ trÃ¬nh debug
1. âŒ KhÃ´ng test FP32 ngay tá»« Ä‘áº§u (máº¥t 30 phÃºt)
2. âŒ Sá»­a nhiá»u thá»© cÃ¹ng lÃºc thay vÃ¬ tá»«ng cÃ¡i má»™t
3. âŒ KhÃ´ng cÃ³ debug logging tá»« Ä‘áº§u
4. âŒ KhÃ´ng verify trainable parameters

### NÃªn lÃ m gÃ¬
1. âœ“ **BÆ°á»›c Ä‘áº§u tiÃªn**: Táº¯t FP16, test vá»›i FP32
2. âœ“ **BÆ°á»›c hai**: ThÃªm debug logging á»Ÿ cÃ¡c checkpoints quan trá»ng
3. âœ“ **BÆ°á»›c ba**: Binary search Ä‘á»ƒ tÃ¬m module cÃ³ váº¥n Ä‘á»
4. âœ“ **BÆ°á»›c bá»‘n**: Kiá»ƒm tra edge cases trong loss computation

---

## ğŸ”§ Quick Reference Commands

```bash
# Debug mode: giáº£m batch size, tÄƒng logging
python train.py --per_device_train_batch_size 1 \
                --logging_steps 1 \
                --max_steps 10 \
                --fp16 false

# Check GPU memory
nvidia-smi --query-gpu=memory.used,memory.free --format=csv -l 1

# Monitor training loss real-time
tail -f logs/train.log | grep -E "(loss|nan|inf)"
```

---

*Last updated: December 2025*
*Generated from debugging session: Instruct-TTS-Chatterbox NaN loss issue*

